{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Data dir: /dati/luca/Uni-Luca/Tesi/progetto/datasets\n",
      "      Tuner dir: /dati/luca/Uni-Luca/Tesi/progetto/models\n",
      "Tensorboard dir: /tmp/tensorboard\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from functools import *\n",
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + os.sep + os.pardir)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from lib.time_series_datasets import *\n",
    "from lib.models import *\n",
    "from lib.benchmarks import *\n",
    "import notify2  # TODO replace notify watch here : https://notify2.readthedocs.io/en/latest/\n",
    "from tensorflow import keras\n",
    "\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"datasets\")\n",
    "TUNER_ROOT = os.path.join(PROJECT_ROOT, \"models\")\n",
    "BENCHMARKS_ROOT = os.path.join(PROJECT_ROOT, \"plots\", \"benchmarks\")\n",
    "WEIGHTS_ROOT = os.path.join(PROJECT_ROOT, \"plots\", \"weights\")\n",
    "TB_ROOT = os.path.join(os.path.abspath(os.sep), \"tmp\", \"tensorboard\")\n",
    "\n",
    "BENCHMARKS_DIR = \"benchmarks\"\n",
    "WEIGHTS_DIR = \"weights\"\n",
    "\n",
    "print(\"       Data dir:\", DATA_ROOT)\n",
    "print(\"      Tuner dir:\", TUNER_ROOT)\n",
    "print(\"Tensorboard dir:\", TB_ROOT)\n",
    "\n",
    "SKIP = False  # Skip all if a model is already tested?\n",
    "OVERWRITE = False  # Redoing the model selection for a model?\n",
    "BENCHMARKS_TRIALS = 10  # How many times do the benchmark. 0 to skip bechmark\n",
    "\n",
    "TUNER = \"BayesianOptimization\"  # \"Hyperband\" or \"BayesianOptimization\"\n",
    "\n",
    "MAX_UNITS = 400\n",
    "MAX_EPOCHS = 200\n",
    "PATIENCE = 10 # EarlyStopping\n",
    "TRIALS = 1  # int positive number of iteration for one set of hyperparameter\n",
    "\n",
    "MAX_TRIALS = 100  # BayesianOptimization\n",
    "\n",
    "#set the following values based on the specific dataset\n",
    "READOUT_ACTIVATION = keras.activations.softmax  # https://www.tensorflow.org/api_docs/python/tf/keras/activations  'softmax'\n",
    "LOSS_FUNCTION = keras.losses.SparseCategoricalCrossentropy()  # https://www.tensorflow.org/api_docs/python/tf/keras/losses  'sparse_categorical_crossentropy'\n",
    "\n",
    "if not os.path.exists(TUNER_ROOT):\n",
    "    os.makedirs(TUNER_ROOT)\n",
    "if not os.path.exists(TB_ROOT):\n",
    "    os.makedirs(TB_ROOT)\n",
    "\n",
    "benchmarks = BenchmarksDB(load_path=os.path.join(BENCHMARKS_ROOT, \"benchmarks.json\"), plot_path=WEIGHTS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|        | ArticularyWordRecognition | CharacterTrajectories | Epilepsy | JapaneseVowels  | Libras | SpokenArabicDigits |\n",
    "|--------|:-------------------------:|:---------------------:|:--------:|:---------------:|:------:|:------------------:|\n",
    "| Input  |             9             |           3           |    3     |       12        |   2    |         13         |\n",
    "| Output |            25             |          20           |    4     |        9        |   15   |         10         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from keras_tuner import Hyperband, BayesianOptimization\n",
    "\n",
    "from IPython import display as ids\n",
    "from keras import callbacks as kc\n",
    "\n",
    "class CurrentMSModel(kc.Callback):\n",
    "    def __init__(self, names):\n",
    "        super().__init__()\n",
    "        self.dataset_name, self.class_name, self.experiment_name, self.model_name = names\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        ids.clear_output(wait=True)\n",
    "        print(\"MS of {} {} {} {}\".format(self.dataset_name, self.class_name, self.experiment_name, self.model_name))\n",
    "\n",
    "def model_selection(build_model_fn, names,\n",
    "                    train_set, val_set,\n",
    "                    tuner_path, verbose=1):\n",
    "    dataset_name, class_name, experiment_name, model_name = names\n",
    "    x_train, y_train = train_set\n",
    "    x_val, y_val = val_set\n",
    "    if TUNER == \"Hyperband\":\n",
    "        working_dir = os.path.join(tuner_path, \"Hyperband\", dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = Hyperband(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            hyperband_iterations=1.,\n",
    "            seed=42,\n",
    "\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    elif TUNER == \"BayesianOptimization\":\n",
    "        working_dir = os.path.join(tuner_path, \"BayesianOptimization\", dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = BayesianOptimization(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=MAX_TRIALS,\n",
    "            #num_initial_points=2,\n",
    "            seed=42,\n",
    "\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Tuner -> {}\".format(TUNER))\n",
    "\n",
    "    # choose the best hyperparameters\n",
    "    tuner.search(x_train, y_train, epochs=MAX_EPOCHS, validation_data=(x_val, y_val),\n",
    "                 callbacks=[\n",
    "                     keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE),\n",
    "                     CurrentMSModel(names)\n",
    "                 ], verbose=verbose)\n",
    "    return tuner\n",
    "\n",
    "def testing_model(names, tuner, test_set,\n",
    "                  tensorboard_path=None, benchmarks_verbose=0):\n",
    "    dataset_name, class_name, experiment_name, model_name = names\n",
    "    x_test, y_test = test_set  # keras.callbacks.CallbackList([])\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)]\n",
    "    if tensorboard_path is not None:\n",
    "        tensorboard_dir = tensorboard_path + model_name\n",
    "        callbacks.append(keras.callbacks.TensorBoard(tensorboard_dir, profile_batch='500,500'))\n",
    "\n",
    "    print(\"Start {} benchmarks for {} | {} | {} | {}:\".format(BENCHMARKS_TRIALS, dataset_name, class_name, experiment_name,\n",
    "                                                              model_name))\n",
    "\n",
    "    best_model_hp = tuner.get_best_hyperparameters()[0]\n",
    "    test_model = None\n",
    "    best_acc = 0\n",
    "    best_tested = None\n",
    "    metrics_ts = []\n",
    "    loss_ts = []\n",
    "    required_time = []\n",
    "\n",
    "    tf.random.set_seed(42)\n",
    "\n",
    "    for i in range(BENCHMARKS_TRIALS):\n",
    "        initial_time = time()\n",
    "\n",
    "        test_model = tuner.hypermodel.build(best_model_hp)\n",
    "        test_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=MAX_EPOCHS,\n",
    "                       callbacks=callbacks, verbose=benchmarks_verbose)\n",
    "        loss, metrics = test_model.evaluate(x_train, y_train)\n",
    "        if metrics > best_acc:\n",
    "            best_acc = metrics\n",
    "            best_tested = test_model\n",
    "        required_time.append(time() - initial_time)\n",
    "        metrics_ts.append(metrics)\n",
    "        loss_ts.append(loss)\n",
    "\n",
    "    stat = Statistic(best_model_hp, metrics_ts, loss_ts, required_time)\n",
    "\n",
    "    return best_tested, stat\n",
    "\n",
    "\n",
    "notify2_init = False\n",
    "\n",
    "\n",
    "def send_notification(title, message):\n",
    "    def notify2_init_fun():\n",
    "        global notify2_init\n",
    "        if not notify2_init:\n",
    "            notify2.init(\"Tesi\")\n",
    "\n",
    "    notify2_init_fun()\n",
    "\n",
    "    notice = notify2.Notification(title, message)\n",
    "    notice.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Local Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is a little nightmare\n",
    "\n",
    "class HP:\n",
    "    FREE = 0\n",
    "    FIXED = 1\n",
    "    RESTRICTED = 2\n",
    "\n",
    "    def __init__(self, hp_type, value=None):\n",
    "        self.value = value\n",
    "        self.type = hp_type\n",
    "\n",
    "    @classmethod\n",
    "    def free(cls):\n",
    "        return cls(HP.FREE)\n",
    "\n",
    "    @classmethod\n",
    "    def fixed(cls, value):\n",
    "        return cls(HP.FIXED, value)\n",
    "\n",
    "    @classmethod\n",
    "    def restricted(cls, value=None):\n",
    "        return cls(HP.RESTRICTED, value)\n",
    "\n",
    "\n",
    "def get_int(tuner, name, hp, min_value, max_value, step=1, sampling=None, pn=None, pv=None):\n",
    "    if hp.type == HP.FREE or hp.type == HP.RESTRICTED:\n",
    "        tmp = tuner.Int(name, min_value=min_value, max_value=max_value, step=step, sampling=sampling, parent_name=pn,\n",
    "                        parent_values=pv)\n",
    "    elif hp.type == HP.FIXED:\n",
    "        tmp = tuner.Fixed(name, hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_float(tuner, name, hp, min_value, max_value, step=None, sampling=None, pn=None, pv=None):\n",
    "    if hp.type == HP.FREE or (hp.type == HP.RESTRICTED and hp.value is None):\n",
    "        tmp = tuner.Float(name, min_value=min_value, max_value=max_value, step=step, sampling=sampling, parent_name=pn,\n",
    "                          parent_values=pv)\n",
    "    elif hp.type == HP.FIXED or (hp.type == HP.RESTRICTED and hp.value is not None):\n",
    "        tmp = tuner.Fixed(name, hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_bool(tuner, name, hp):\n",
    "    if hp.type == HP.FREE or hp.type == HP.RESTRICTED:\n",
    "        tmp = tuner.Boolean(name)\n",
    "    elif hp.type == HP.FIXED:\n",
    "        tmp = tuner.Fixed(name, hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_float_vec(tuner, name, hp, length, min_value, max_value, step=None, sampling=None, pn=None, pv=None):\n",
    "    if hp.type == HP.FREE:\n",
    "        tmp = [tuner.Float(name + ' ' + str(i), min_value=min_value, max_value=max_value, sampling=sampling,\n",
    "                           parent_name=pn, parent_values=pv, step=step)\n",
    "               for i in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        tmp = [tuner.Fixed(name + ' ' + str(i), hp.value[i]) for i in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        tmp2 = tuner.Float(name + ' 0', min_value=min_value, max_value=max_value, sampling=sampling, parent_name=pn,\n",
    "                               parent_values=pv, step=step)\n",
    "        tmp = [tmp2 for _ in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_connectivity_esn1(tuner, hp):\n",
    "    if hp.type == HP.FREE:\n",
    "        tmp = tuner.Float('connectivity 0', min_value=0.1, max_value=1., step=0.1)\n",
    "    elif hp.type == HP.FIXED:\n",
    "        raise ValueError(\"HP.FIXED not valid for connectivity\")\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        tmp = tuner.Fixed('connectivity 0', hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def get_connectivity_esn2(tuner, hp, length):\n",
    "    if hp.type == HP.FREE:\n",
    "        tmp = [tuner.Float('connectivity ' + str(i), min_value=0., max_value=1.) for i in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        raise ValueError(\"HP.FIXED not valid for connectivity\")\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        if hp.value is None:\n",
    "            tmp2 = tuner.Float('connectivity 0', min_value=0., max_value=1.)\n",
    "        else:\n",
    "            tmp2 = tuner.Fixed('connectivity 0', hp.value)\n",
    "        tmp = [tmp2 for _ in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_connectivity(tuner, hp, length):  # It is a squared matrix\n",
    "    if hp.type == HP.FREE:\n",
    "        conn_matrix = [[tuner.Float('connectivity ' + str(i), min_value=0., max_value=1., step = 0.1) if i == j else\n",
    "                        tuner.Float('connectivity ' + str(i) + '->' + str(j), min_value=0., max_value=1., step = 0.1)\n",
    "                        for i in range(length)]\n",
    "                       for j in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        diagonal, off_diagonal = hp.value\n",
    "        off_diagonal = tuner.Fixed('connectivity X->Y', off_diagonal)\n",
    "        conn_matrix = [[tuner.Fixed('connectivity ' + str(i), diagonal) if i == j else\n",
    "                        off_diagonal\n",
    "                        for i in range(length)]\n",
    "                       for j in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        if hp.value is None:\n",
    "            connectivity = [tuner.Float('connectivity ' + str(i), min_value=0., max_value=1., step=0.1) for i in range(length)]\n",
    "        else:\n",
    "            tmp = tuner.Fixed('connectivity 0', hp.value)\n",
    "            connectivity = [ tmp for _ in range(length)]\n",
    "        intra_connectivity = tuner.Float('connectivity X->Y', min_value=0., max_value=1., step=0.1)\n",
    "        conn_matrix = [[connectivity[i] if i == j else intra_connectivity for i in range(length)] for j in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return conn_matrix\n",
    "\n",
    "def get_minmax(tuner, hp, length):  # Is a matrix for ESN3 and ESN4\n",
    "    max_value = 2.0\n",
    "    if hp.type == HP.FREE:\n",
    "        minmax_vec = [[0. if i == j else tuner.Float('minmax ' + str(i) + '->' + str(j), min_value=0.1, max_value=max_value, step = 0.1)\n",
    "                   for i in range(length)]\n",
    "                  for j in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        minmax = tuner.Float('minmax', min_value=0.1, max_value=max_value, step = 0.1)\n",
    "        minmax_vec = [[0. if i == j else minmax\n",
    "                   for i in range(length)]\n",
    "                  for j in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        minmax = hp.value\n",
    "        minmax_vec = [[0. if i == j else minmax\n",
    "                   for i in range(length)]\n",
    "                  for j in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return minmax_vec\n",
    "\n",
    "def get_spectral_radius(tuner, hp, length):  # Is a vector\n",
    "    max_value = 2.0\n",
    "    if hp.type == HP.FREE:\n",
    "        sr_vec = [tuner.Float('spectral radius ' + str(i), min_value=0.1, max_value=max_value, step = 0.1) for i in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        spectral_radius = tuner.Float('spectral radius 0', min_value=0.1, max_value=max_value, step = 0.1)\n",
    "        sr_vec = [spectral_radius for _ in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        spectral_radius = hp.value\n",
    "        sr_vec = [spectral_radius for _ in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return sr_vec\n",
    "\n",
    "def get_gsr(tuner, gsr):\n",
    "    global_sr = get_bool(tuner, 'use G.S.R.', gsr)\n",
    "    if global_sr:\n",
    "        global_sr = tuner.Float('G.S.R.', min_value=0.1, max_value=2., parent_name='use G.S.R.', parent_values=True, step = 0.1)\n",
    "    else:\n",
    "        global_sr = None #  tuner.Fixed('G.S.R.', False, parent_name='use G.S.R.', parent_values=False)\n",
    "\n",
    "    return global_sr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build model functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_ESN1(output, _reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, _gsr, _off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,  # Defined by experiment\n",
    "               tuner) -> ESN1:\n",
    "    tmp_model = ESN1(units=get_int(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     connectivity=get_connectivity_esn1(tuner, connectivity),\n",
    "                     spectral_radius=get_float(tuner, 'spectral radius 0', spectral_radius, min_value=0.1, max_value=2., step = 0.1),\n",
    "                     output_units=output,\n",
    "                     readout_activation=READOUT_ACTIVATION,\n",
    "                     input_scaling=get_float(tuner, 'input scaling 0', input_scaling, min_value=0.1, max_value=1.5, step=0.1),\n",
    "                     bias_scaling=get_float(tuner, 'bias scaling 0', bias_scaling, min_value=0.1, max_value=1.5, step=0.1),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., step=0.1),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=LOSS_FUNCTION,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_ESN2(output, reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, gsr, _off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,\n",
    "               # Defined by experiment\n",
    "               tuner) -> ESN2:\n",
    "    tmp_model = ESN2(units=get_int(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     sub_reservoirs=reservoirs,\n",
    "                     connectivity=get_connectivity_esn2(tuner, connectivity, reservoirs),\n",
    "                     spectral_radius=get_spectral_radius(tuner, spectral_radius, reservoirs),\n",
    "                     gsr=get_gsr(tuner, gsr),\n",
    "                     output_units=output,\n",
    "                     readout_activation=READOUT_ACTIVATION,\n",
    "                     input_scaling=get_float_vec(tuner, 'input scaling', input_scaling, reservoirs, min_value=0.1,\n",
    "                                                 max_value=1.5, step=0.1),\n",
    "                     bias_scaling=get_float_vec(tuner, 'bias scaling', bias_scaling, reservoirs, min_value=0.1,\n",
    "                                                max_value=1.5, step=0.1),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., step=0.1),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=LOSS_FUNCTION,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_ESN3(output, reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, gsr, off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,\n",
    "               # Defined by experiment\n",
    "               tuner) -> ESN3:\n",
    "    tmp_model = ESN3(units=get_int(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     sub_reservoirs=reservoirs,\n",
    "                     connectivity=get_connectivity(tuner, connectivity, reservoirs),\n",
    "                     spectral_radius=get_spectral_radius(tuner, spectral_radius, reservoirs),\n",
    "                     gsr=get_gsr(tuner, gsr),\n",
    "                     off_diagonal=get_minmax(tuner, off_diag, reservoirs),\n",
    "                     output_units=output,\n",
    "                     readout_activation=READOUT_ACTIVATION,\n",
    "                     input_scaling=get_float_vec(tuner, 'input scaling', input_scaling, reservoirs, min_value=0.1,\n",
    "                                                 max_value=1.5, step=0.1),\n",
    "                     bias_scaling=get_float_vec(tuner, 'bias scaling', bias_scaling, reservoirs, min_value=0.1,\n",
    "                                                max_value=1.5, step=0.1),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., step=0.1),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=LOSS_FUNCTION,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_ESN4(output, reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, gsr, off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,\n",
    "               # Defined by experiment\n",
    "               tuner) -> ESN4:\n",
    "    partitions = [tuner.Float('partition ' + str(i), min_value=0.1, max_value=1.0, step=0.333) for i in range(reservoirs)]\n",
    "    total = sum(partitions)\n",
    "    # Normalize the partition vector now sum(partitions) == 1.\n",
    "    partitions = list(map(lambda _x: 0 if total == 0 else _x / total, partitions))\n",
    "    tmp_model = ESN4(units=get_int(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     sub_reservoirs=reservoirs,\n",
    "                     connectivity=get_connectivity(tuner, connectivity, reservoirs),\n",
    "                     partitions=partitions,\n",
    "                     spectral_radius=get_spectral_radius(tuner, spectral_radius, reservoirs),\n",
    "                     gsr=get_gsr(tuner, gsr),\n",
    "                     off_diagonal=get_minmax(tuner, off_diag, reservoirs),\n",
    "                     output_units=output,\n",
    "                     readout_activation=READOUT_ACTIVATION,\n",
    "                     input_scaling=get_float_vec(tuner, 'input scaling', input_scaling, reservoirs, min_value=0.1,\n",
    "                                                 max_value=1.5, step=0.1),\n",
    "                     bias_scaling=get_float_vec(tuner, 'bias scaling', bias_scaling, reservoirs, min_value=0.1,\n",
    "                                                max_value=1.5, step=0.1),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., step=0.1),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),# keras.optimizers.RMSprop(alpha),\n",
    "        loss=LOSS_FUNCTION,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return tmp_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Datasets': [\n",
    "        # \"ArticularyWordRecognition\",\n",
    "        \"CharacterTrajectories\",\n",
    "        # \"Libras\",\n",
    "        # \"SpokenArabicDigits\",\n",
    "        # \"Epilepsy\",\n",
    "        # \"JapaneseVowels\"\n",
    "    ],\n",
    "    'Classes': [\n",
    "        # \"Best Models\",\n",
    "        \"Multiple S.R.\",\n",
    "        # \"Single S.R.\"\n",
    "    ],\n",
    "    'Models': [\n",
    "        build_ESN1,\n",
    "        build_ESN2,\n",
    "        build_ESN3,\n",
    "        build_ESN4\n",
    "    ],\n",
    "    #                Units         | Spectral radius | G.S.R.    | off-diag      | Connectivity     | Input scaling  | Bias scaling   | leaky    | learning rate\n",
    "    #                F             | F-R             | F         | F - R         | F - R            | F-R            | F-R            | F        | F\n",
    "    'Best Models': {\n",
    "        'Best':      (HP.free(),     HP.free(),      HP.free(),  HP.restricted(), HP.restricted(),   HP.free(),       HP.free(),       HP.free(), HP.free())\n",
    "    },\n",
    "    'Multiple S.R.': {  # Modello con N sub res ora ha 7 + N ( N diag sr + 1 off diag st, 1 g.s.r., 1 off-diag conn, 1 input, 1 bias, 1 leaky, 1 learning)\n",
    "         'Units 50': (HP.fixed(50),  HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "         'Units 75': (HP.fixed(75),  HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 100': (HP.fixed(100), HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 150': (HP.fixed(150), HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 250': (HP.fixed(250), HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "    },\n",
    "    'Single S.R.': {\n",
    "         'Units 50': (HP.fixed(50),  HP.restricted(), HP.free(), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "         'Units 75': (HP.fixed(75),  HP.restricted(), HP.free(), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 100': (HP.fixed(100), HP.restricted(), HP.free(), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 150': (HP.fixed(150), HP.restricted(), HP.free(), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 250': (HP.fixed(250), HP.restricted(), HP.free(), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "datasets = config.get('Datasets')\n",
    "classes = config.get('Classes')\n",
    "models_fn = config.get('Models')\n",
    "\n",
    "run_time = time()\n",
    "for dataset in datasets:\n",
    "    train_path = os.path.join(DATA_ROOT, dataset, dataset + '_TRAIN.ts')\n",
    "    test_path = os.path.join(DATA_ROOT, dataset, dataset + '_TEST.ts')\n",
    "\n",
    "    x_train_all, y_train_all = load_sktime_dataset(train_path)\n",
    "    x_test, y_test = load_sktime_dataset(test_path)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all,\n",
    "                                                      test_size=0.33, random_state=42, stratify=y_train_all)\n",
    "\n",
    "    train_set = (x_train, y_train)  # Todo is this cast necessary?\n",
    "    val_set = (x_val, y_val)\n",
    "    test_set = (x_test, y_test)\n",
    "\n",
    "    features = x_train.shape[-1]\n",
    "    output_units = len(np.unique(y_test))  # Dataset must have one of each features\n",
    "\n",
    "    for class_name in classes:\n",
    "        class_time = time()\n",
    "        for experiment, params in config.get(class_name).items():\n",
    "            for model_fn in models_fn:\n",
    "                model_name = model_fn.__annotations__['return'].__name__\n",
    "                already_tested = benchmarks.is_benchmarked(dataset, class_name, experiment, model_name)\n",
    "\n",
    "                if already_tested and SKIP:\n",
    "                    continue\n",
    "\n",
    "                build_fn = partial(model_fn, output_units, features, *params)\n",
    "                names = (dataset, class_name, experiment, model_name)\n",
    "\n",
    "                tuner = model_selection(build_fn, names,\n",
    "                                        train_set, val_set,\n",
    "                                        tuner_path=TUNER_ROOT, verbose=1)\n",
    "\n",
    "                if BENCHMARKS_TRIALS > 0:\n",
    "                    model, stat = testing_model(names, tuner, test_set)\n",
    "                    benchmarks.add(dataset, class_name, experiment, model_name, stat)\n",
    "                    #model.plot(names, path=WEIGHTS_ROOT, show=False)\n",
    "\n",
    "        # benchmarks.save()\n",
    "        # send_notification(class_name + \" done\", \"Requested time:\" + str(time() - class_time))\n",
    "\n",
    "print(\"Total learning time:\" + str(time() - run_time))\n",
    "send_notification(\"All Done\", \"Requested time:\" + str(time() - run_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "benchmarks.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}