{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Data dir: /dati/luca/Uni-Luca/Tesi/progetto/datasets\n",
      "      Tuner dir: /dati/luca/Uni-Luca/Tesi/progetto/models\n",
      " Benchmarks dir: /dati/luca/Uni-Luca/Tesi/progetto/benchmarks\n",
      "    Weights dir: /dati/luca/Uni-Luca/Tesi/progetto/plots/weights\n",
      "Tensorboard dir: /tmp/tensorboard\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/datatypes/_series/_check.py:42: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/datatypes/_panel/_check.py:45: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/datatypes/_panel/_check.py:46: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from functools import *\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from time import time\n",
    "from keras_tuner import Hyperband, BayesianOptimization, RandomSearch\n",
    "from keras import callbacks as kc\n",
    "\n",
    "from IRESNs_tensorflow.time_series_datasets import *\n",
    "from IRESNs_tensorflow.models import *\n",
    "from benchmarks import *\n",
    "from general_hp import *\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + os.sep + os.pardir)\n",
    "\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"datasets\")\n",
    "TUNER_ROOT = os.path.join(PROJECT_ROOT, \"models\")\n",
    "TB_ROOT = os.path.join(os.path.abspath(os.sep), \"tmp\", \"tensorboard\")\n",
    "\n",
    "BENCHMARKS_ROOT = os.path.join(PROJECT_ROOT, \"benchmarks\")\n",
    "WEIGHTS_ROOT = os.path.join(PROJECT_ROOT, \"plots\", \"weights\")\n",
    "\n",
    "print(\"       Data dir:\", DATA_ROOT)\n",
    "print(\"      Tuner dir:\", TUNER_ROOT)\n",
    "print(\" Benchmarks dir:\", BENCHMARKS_ROOT)\n",
    "print(\"    Weights dir:\", WEIGHTS_ROOT)\n",
    "print(\"Tensorboard dir:\", TB_ROOT)\n",
    "\n",
    "TUNER = \"RandomSearch\"  # \"Hyperband\" or \"BayesianOptimization\" or \"RandomSearch\"\n",
    "\n",
    "CONFIGURATIONS = 5  # Positive Integer. How many configuration the tuner will try\n",
    "MAX_EPOCHS = 5  # Positive Integer. How many epochs the tuner train the model for each trials\n",
    "TRIALS = 1  # Positive Integer. How many iterations for one set of hyperparameters\n",
    "PATIENCE = 10  # EarlyStopping\n",
    "BENCHMARKS_TRIALS = 2  # How many times do the benchmark. 0 to skip BENCHMARKS\n",
    "MS_VERBOSE = 0\n",
    "\n",
    "MAX_UNITS = 250\n",
    "UNITS = sorted([50, 75, 100, 150, 250])\n",
    "MINVAL = 0.01  # Positive Float. How\n",
    "MAXVAL = 1.5\n",
    "\n",
    "SKIP = False  # Skip if a model is already tested?\n",
    "OVERWRITE = True  # Redo the model selection for a model?\n",
    "\n",
    "TUNER_DESC = \"Esperimenti\"\n",
    "\n",
    "READOUT_ACTIVATION_BINARY = keras.activations.sigmoid\n",
    "LOSS_FUNCTION_BINARY = keras.losses.BinaryCrossentropy()\n",
    "READOUT_ACTIVATION = keras.activations.softmax  # https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "LOSS_FUNCTION = keras.losses.SparseCategoricalCrossentropy()  # https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "\n",
    "if not os.path.exists(TUNER_ROOT):\n",
    "    os.makedirs(TUNER_ROOT)\n",
    "if not os.path.exists(TB_ROOT):\n",
    "    os.makedirs(TB_ROOT)\n",
    "\n",
    "TUNER_STRING = TUNER + \".\" + str(MAX_EPOCHS) + \"me\" + str(CONFIGURATIONS) + \"mt.\" + TUNER_DESC\n",
    "benchmarks = BenchmarksDB(load_path=os.path.join(BENCHMARKS_ROOT, TUNER_STRING + \".json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CurrentMSModel(kc.Callback):\n",
    "    def __init__(self, names):\n",
    "        super().__init__()\n",
    "        self.dataset_name, self.class_name, self.experiment_name, self.model_name = names\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        #ids.clear_output(wait=True)\n",
    "        print(\"MS of {} {} {} {}\".format(self.dataset_name, self.class_name, self.experiment_name, self.model_name))\n",
    "\n",
    "\n",
    "def get_seed(names):\n",
    "    dataset_name, class_name, experiment_name, model_name = names\n",
    "    import hashlib\n",
    "    union = dataset_name + class_name + experiment_name + model_name\n",
    "    hashed = hashlib.md5(union.encode('UTF-8'))\n",
    "    seed = int(hashed.hexdigest(), 16) % 4294967295  # limit to 32 bit length value\n",
    "    return seed\n",
    "\n",
    "\n",
    "def model_selection(build_model_fn, names,\n",
    "                    train_set, val_set,\n",
    "                    tuner_path, verbose=1):\n",
    "    dataset_name, class_name, experiment_name, model_name = names\n",
    "    x_train, y_train = train_set\n",
    "    x_val, y_val = val_set\n",
    "    seed = get_seed(names)\n",
    "    if TUNER == \"Hyperband\":\n",
    "        working_dir = os.path.join(tuner_path, TUNER_STRING, dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = Hyperband(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            hyperband_iterations=1.,\n",
    "            seed=seed,\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    elif TUNER == \"BayesianOptimization\":\n",
    "        working_dir = os.path.join(tuner_path, TUNER_STRING, dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = BayesianOptimization(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=CONFIGURATIONS,\n",
    "            seed=seed,\n",
    "\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    elif TUNER == \"RandomSearch\":\n",
    "        working_dir = os.path.join(tuner_path, TUNER_STRING, dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = RandomSearch(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=CONFIGURATIONS,\n",
    "            seed=seed,\n",
    "\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Tuner -> {}\".format(TUNER))\n",
    "\n",
    "    # now the tuner will search the best hyperparameters\n",
    "    tuner.search(x_train, y_train, epochs=MAX_EPOCHS, validation_data=(x_val, y_val),\n",
    "                 callbacks=[\n",
    "                     keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE),\n",
    "                     #CurrentMSModel(names)\n",
    "                 ], verbose=verbose)\n",
    "\n",
    "    return tuner, tuner.oracle.get_best_trials(1)[0].score\n",
    "\n",
    "\n",
    "def testing_model(names, tuner,\n",
    "                  train_set, val_set, test_set,\n",
    "                  tensorboard_path=None, benchmarks_verbose=0):\n",
    "    dataset_name, class_name, experiment_name, model_name = names\n",
    "    x_train, y_train = train_set\n",
    "    x_val, y_val = val_set\n",
    "    x_test, y_test = test_set\n",
    "\n",
    "    # keras.callbacks.CallbackList([])\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)]\n",
    "    if tensorboard_path is not None:\n",
    "        tensorboard_dir = tensorboard_path + model_name\n",
    "        callbacks.append(keras.callbacks.TensorBoard(tensorboard_dir, profile_batch='500,500'))\n",
    "\n",
    "    print(\"[{}] Running {} benchmarks\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), BENCHMARKS_TRIALS))\n",
    "\n",
    "    best_model_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    test_model = None\n",
    "\n",
    "    required_time = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "    tf.random.set_seed(get_seed(names))\n",
    "\n",
    "    for i in range(BENCHMARKS_TRIALS):\n",
    "        initial_time = time()\n",
    "\n",
    "        test_model = tuner.hypermodel.build(best_model_hp)\n",
    "        history = test_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=MAX_EPOCHS,\n",
    "                                 # perchè si usa il validation data?\n",
    "                                 callbacks=callbacks, verbose=benchmarks_verbose)\n",
    "        test_loss, accuracy = test_model.evaluate(x_test, y_test)\n",
    "\n",
    "        required_time.append(time() - initial_time)\n",
    "\n",
    "        train_acc.append(history.history['accuracy'][-1])\n",
    "        val_acc.append(history.history['val_accuracy'][-1])\n",
    "        test_acc.append(accuracy)\n",
    "\n",
    "    stat = Statistic(best_model_hp, train_acc, val_acc, test_acc, required_time)\n",
    "\n",
    "    return test_model, stat\n",
    "\n",
    "\n",
    "import notify2  # TODO replace notify watch here : https://notify2.readthedocs.io/en/latest/\n",
    "\n",
    "notify2_init = False\n",
    "\n",
    "\n",
    "def send_notification(title, message):\n",
    "    def notify2_init_fun():\n",
    "        global notify2_init\n",
    "        if not notify2_init:\n",
    "            notify2.init(\"Tesi\")\n",
    "\n",
    "    notify2_init_fun()\n",
    "\n",
    "    notice = notify2.Notification(title, message)\n",
    "    notice.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build model functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def build_ESN(output_classes, _reservoirs,  # Defined by dataset\n",
    "              hp, tuner) -> ESN:\n",
    "    if output_classes == 2:\n",
    "        output_units = 1\n",
    "        readout_activation = READOUT_ACTIVATION_BINARY\n",
    "        loss = LOSS_FUNCTION_BINARY\n",
    "    else:\n",
    "        output_units = output_classes\n",
    "        readout_activation = READOUT_ACTIVATION\n",
    "        loss = LOSS_FUNCTION\n",
    "\n",
    "    tmp_model = ESN(units=hp.get_units(tuner, min_value=50, max_value=MAX_UNITS, choise=UNITS),\n",
    "                    connectivity=hp.get_connectivity_esn(tuner),\n",
    "                    spectral_radius=hp.get_normalization_esn(tuner, min_value=MINVAL, max_value=MAXVAL,\n",
    "                                                             sampling=\"linear\"),\n",
    "                    input_scaling=hp.get_input_scaling(tuner, 1, min_value=MINVAL, max_value=MAXVAL, sampling=\"linear\"),\n",
    "                    bias_scaling=hp.get_bias_scaling(tuner, 1, min_value=MINVAL, max_value=MAXVAL, sampling=\"linear\"),\n",
    "                    leaky=hp.get_leaky(tuner),\n",
    "                    output_units=output_units,\n",
    "                    readout_activation=readout_activation,\n",
    "                    dtype=tf.float32\n",
    "\n",
    "                    )\n",
    "\n",
    "    alpha = hp.get_learning_rate(tuner)\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_IRESN(output_classes, reservoirs,  # Defined by dataset\n",
    "                hp, tuner) -> IRESN:\n",
    "    if output_classes == 2:\n",
    "        output_units = 1\n",
    "        readout_activation = READOUT_ACTIVATION_BINARY\n",
    "        loss = LOSS_FUNCTION_BINARY\n",
    "    else:\n",
    "        output_units = output_classes\n",
    "        readout_activation = READOUT_ACTIVATION\n",
    "        loss = LOSS_FUNCTION\n",
    "\n",
    "    tmp_model = IRESN(units=hp.get_units(tuner, min_value=50, max_value=MAX_UNITS, choise=UNITS),\n",
    "                      sub_reservoirs=reservoirs,\n",
    "                      connectivity=hp.get_connectivity_iresn(tuner, reservoirs),\n",
    "                      normalization=hp.get_normalization_iresn(tuner, reservoirs, min_value=MINVAL,\n",
    "                                                                 max_value=MAXVAL),\n",
    "                      input_scaling=hp.get_input_scaling(tuner, reservoirs, min_value=MINVAL,\n",
    "                                                         max_value=MAXVAL, sampling=\"linear\"),\n",
    "                      bias_scaling=hp.get_bias_scaling(tuner, reservoirs, min_value=MINVAL, max_value=MAXVAL,\n",
    "                                                       sampling=\"linear\"),\n",
    "                      leaky=hp.get_leaky(tuner),\n",
    "                      gsr=hp.get_gsr(tuner, min_value=MINVAL, max_value=MAXVAL),\n",
    "                      vsr=hp.get_vsr(tuner, reservoirs),\n",
    "                      output_units=output_units,\n",
    "                      readout_activation=readout_activation,\n",
    "                      dtype=tf.float32\n",
    "                      )\n",
    "\n",
    "    alpha = hp.get_learning_rate(tuner)\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_IIRESN(output_classes, reservoirs,  # Defined by dataset\n",
    "                 hp, tuner) -> IIRESN:\n",
    "    if output_classes == 2:\n",
    "        output_units = 1\n",
    "        readout_activation = READOUT_ACTIVATION_BINARY\n",
    "        loss = LOSS_FUNCTION_BINARY\n",
    "    else:\n",
    "        output_units = output_classes\n",
    "        readout_activation = READOUT_ACTIVATION\n",
    "        loss = LOSS_FUNCTION\n",
    "\n",
    "    tmp_model = IIRESN(units=hp.get_units(tuner, min_value=50, max_value=MAX_UNITS, choise=UNITS),\n",
    "                       sub_reservoirs=reservoirs,\n",
    "                       connectivity=hp.get_connectivity_iiresn(tuner, reservoirs),\n",
    "                       normalization=hp.get_normalization_iiresn(tuner, reservoirs),\n",
    "                       use_norm2=hp.use_norm2,\n",
    "                       input_scaling=hp.get_input_scaling(tuner, reservoirs, min_value=MINVAL,\n",
    "                                                          max_value=MAXVAL, sampling=\"linear\"),\n",
    "                       bias_scaling=hp.get_bias_scaling(tuner, reservoirs, min_value=MINVAL,\n",
    "                                                        max_value=MAXVAL, sampling=\"linear\"),\n",
    "                       leaky=hp.get_leaky(tuner),\n",
    "                       gsr=hp.get_gsr(tuner, min_value=MINVAL, max_value=MAXVAL),\n",
    "                       vsr=hp.get_vsr(tuner, reservoirs),\n",
    "                       output_units=output_units,\n",
    "                       readout_activation=readout_activation,\n",
    "                       dtype=tf.float32\n",
    "                       )\n",
    "\n",
    "    alpha = hp.get_learning_rate(tuner)\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiugrations\n",
    "\n",
    "|        | ArticularyWordRecognition | CharacterTrajectories | Epilepsy | JapaneseVowels  | Libras | SpokenArabicDigits |\n",
    "|--------|:-------------------------:|:---------------------:|:--------:|:---------------:|:------:|:------------------:|\n",
    "| Input  |             9             |           3           |    3     |       12        |   2    |         13         |\n",
    "| Output |            25             |          20           |    4     |        9        |   15   |         10         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def get_name(fn):\n",
    "    return fn.__annotations__['return'].__name__\n",
    "\n",
    "\n",
    "from general_hp import HP, HP_Manager\n",
    "\n",
    "single_sr = HP_Manager(units=HP.fixed(100),\n",
    "                       norm_sub_reservoirs=HP.restricted(),\n",
    "                       norm_inter_connectivity=HP.restricted(),\n",
    "                       connectivity=HP.restricted(1.),\n",
    "                       input_scaling=HP.restricted(),\n",
    "                       bias_scaling=HP.restricted(),\n",
    "                       gsr=HP.fixed(False),\n",
    "                       vsr=HP.fixed(False),\n",
    "                       )\n",
    "single_sr_vsr = HP_Manager(units=HP.fixed(100),\n",
    "                           norm_sub_reservoirs=HP.restricted(),\n",
    "                           norm_inter_connectivity=HP.restricted(),\n",
    "                           connectivity=HP.restricted(1.),\n",
    "                           input_scaling=HP.restricted(),\n",
    "                           bias_scaling=HP.restricted(),\n",
    "                           gsr=HP.fixed(False),\n",
    "                           vsr=HP.fixed(True),\n",
    "                           )\n",
    "multiple_sr = HP_Manager(units=HP.fixed(100),\n",
    "                         norm_sub_reservoirs=HP.free(),\n",
    "                         norm_inter_connectivity=HP.restricted(),\n",
    "                         connectivity=HP.restricted(1.),\n",
    "                         input_scaling=HP.restricted(),\n",
    "                         bias_scaling=HP.restricted(),\n",
    "                         gsr=HP.fixed(False),\n",
    "                         vsr=HP.fixed(False),\n",
    "                         )\n",
    "multiple_sr_vsr = HP_Manager(units=HP.fixed(100),\n",
    "                             norm_sub_reservoirs=HP.free(),\n",
    "                             norm_inter_connectivity=HP.restricted(),\n",
    "                             connectivity=HP.restricted(1.),\n",
    "                             input_scaling=HP.restricted(),\n",
    "                             bias_scaling=HP.restricted(),\n",
    "                             gsr=HP.fixed(False),\n",
    "                             vsr=HP.fixed(True),\n",
    "                             )\n",
    "multiple_sr_multiple_is = HP_Manager(units=HP.fixed(100),\n",
    "                                     norm_sub_reservoirs=HP.free(),\n",
    "                                     norm_inter_connectivity=HP.restricted(),\n",
    "                                     connectivity=HP.restricted(1.),\n",
    "                                     input_scaling=HP.free(),\n",
    "                                     bias_scaling=HP.restricted(),\n",
    "                                     gsr=HP.fixed(False),\n",
    "                                     vsr=HP.fixed(False),\n",
    "                                     )\n",
    "multiple_sr_multiple_is_vsr = HP_Manager(units=HP.fixed(100),\n",
    "                                         norm_sub_reservoirs=HP.free(),\n",
    "                                         norm_inter_connectivity=HP.restricted(),\n",
    "                                         connectivity=HP.restricted(1.),\n",
    "                                         input_scaling=HP.free(),\n",
    "                                         bias_scaling=HP.restricted(),\n",
    "                                         gsr=HP.fixed(False),\n",
    "                                         vsr=HP.fixed(True),\n",
    "                                         )\n",
    "\n",
    "config = {\n",
    "    'Datasets'                   : [\n",
    "        #\"CharacterTrajectories\",\n",
    "        #\"Libras\",\n",
    "        #\"SpokenArabicDigits\",\n",
    "        \"ArticularyWordRecognition\",\n",
    "        #\"Epilepsy\",\n",
    "        #\"JapaneseVowels\"\n",
    "    ],\n",
    "    'Classes'                    : [\n",
    "        #'Reference',\n",
    "        #'Single SR',\n",
    "        #'Single SR vsr',\n",
    "        #'Multiple SR',\n",
    "        #'Multiple SR vsr',\n",
    "        'Multiple SR Multiple IS',\n",
    "        'Multiple SR Multiple IS vsr',\n",
    "    ],\n",
    "    'Reference'                  : {\n",
    "        'Models'     : [\n",
    "            build_ESN\n",
    "        ],\n",
    "        'Experiments': {\n",
    "            'Units 100': single_sr\n",
    "        }\n",
    "    },\n",
    "    'Single SR'                  : {\n",
    "        'Models'     : [\n",
    "            build_IRESN,\n",
    "            build_IIRESN,\n",
    "        ],\n",
    "        'Experiments': {\n",
    "            'Units 100': single_sr,\n",
    "        }\n",
    "    },\n",
    "    'Single SR vsr'              : {\n",
    "        'Models'     : [\n",
    "            build_IRESN,\n",
    "            build_IIRESN,\n",
    "        ],\n",
    "        'Experiments': {\n",
    "            'Units 100': single_sr_vsr,\n",
    "        }\n",
    "    },\n",
    "    'Multiple SR'                : {\n",
    "        'Models'     : [\n",
    "            build_IRESN,\n",
    "            build_IIRESN,\n",
    "        ],\n",
    "        'Experiments': {\n",
    "            'Units 100': multiple_sr\n",
    "        },\n",
    "    },\n",
    "    'Multiple SR vsr'            : {\n",
    "        'Models'     : [\n",
    "            build_IRESN,\n",
    "            build_IIRESN,\n",
    "        ],\n",
    "        'Experiments': {\n",
    "            'Units 100': multiple_sr_vsr\n",
    "        },\n",
    "    },\n",
    "    'Multiple SR Multiple IS'    : {\n",
    "        'Models'     : [\n",
    "            build_IRESN,\n",
    "            build_IIRESN,\n",
    "        ],\n",
    "        'Experiments': {\n",
    "            'Units 100': multiple_sr_multiple_is\n",
    "        },\n",
    "    },\n",
    "    'Multiple SR Multiple IS vsr': {\n",
    "        'Models'     : [\n",
    "            build_IRESN,\n",
    "            build_IIRESN,\n",
    "        ],\n",
    "        'Experiments': {\n",
    "            'Units 100': multiple_sr_multiple_is_vsr\n",
    "        },\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/utils/data_io.py:63: FutureWarning: This function has moved to datasets/_data_io, this version will be removed in V0.10\n",
      "  warn(\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/utils/data_io.py:63: FutureWarning: This function has moved to datasets/_data_io, this version will be removed in V0.10\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/04/2022 18:14:56] M.S. of ArticularyWordRecognition       Reference  Units 100        ESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:14:59] M.S. run time 0:00:03.446285\n",
      "[14/04/2022 18:14:59] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 536us/step - loss: 2.9668 - accuracy: 0.2467\n",
      "10/10 [==============================] - 0s 451us/step - loss: 3.0592 - accuracy: 0.1667\n",
      "[14/04/2022 18:15:03] M.S. of ArticularyWordRecognition       Single SR  Units 100      IRESN\n",
      "WARNING:tensorflow:From /dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/tensorflow/python/ops/linalg/linear_operator_block_diag.py:234: LinearOperator.graph_parents (from tensorflow.python.ops.linalg.linear_operator) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Do not call `graph_parents`.\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:06] M.S. run time 0:00:03.299773\n",
      "[14/04/2022 18:15:06] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 439us/step - loss: 3.6443 - accuracy: 0.0500\n",
      "10/10 [==============================] - 0s 513us/step - loss: 3.5549 - accuracy: 0.0833\n",
      "[14/04/2022 18:15:09] M.S. of ArticularyWordRecognition       Single SR  Units 100     IIRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:13] M.S. run time 0:00:03.554142\n",
      "[14/04/2022 18:15:13] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 496us/step - loss: 1.1251 - accuracy: 0.7167\n",
      "10/10 [==============================] - 0s 436us/step - loss: 1.1141 - accuracy: 0.6867\n",
      "[14/04/2022 18:15:16] M.S. of ArticularyWordRecognition   Single SR vsr  Units 100      IRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:19] M.S. run time 0:00:03.575636\n",
      "[14/04/2022 18:15:19] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 455us/step - loss: 1.0506 - accuracy: 0.7033\n",
      "10/10 [==============================] - 0s 480us/step - loss: 1.0159 - accuracy: 0.7533\n",
      "[14/04/2022 18:15:22] M.S. of ArticularyWordRecognition   Single SR vsr  Units 100     IIRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:26] M.S. run time 0:00:03.520781\n",
      "[14/04/2022 18:15:26] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 480us/step - loss: 1.0135 - accuracy: 0.7267\n",
      "10/10 [==============================] - 0s 423us/step - loss: 1.0823 - accuracy: 0.7133\n",
      "[14/04/2022 18:15:29] M.S. of ArticularyWordRecognition     Multiple SR  Units 100      IRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:33] M.S. run time 0:00:04.271825\n",
      "[14/04/2022 18:15:33] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 447us/step - loss: 0.9251 - accuracy: 0.7167\n",
      "10/10 [==============================] - 0s 458us/step - loss: 0.8925 - accuracy: 0.7367\n",
      "[14/04/2022 18:15:36] M.S. of ArticularyWordRecognition     Multiple SR  Units 100     IIRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:40] M.S. run time 0:00:03.537100\n",
      "[14/04/2022 18:15:40] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 453us/step - loss: 1.0036 - accuracy: 0.7133\n",
      "10/10 [==============================] - 0s 454us/step - loss: 0.9530 - accuracy: 0.7233\n",
      "[14/04/2022 18:15:43] M.S. of ArticularyWordRecognition Multiple SR vsr  Units 100      IRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:46] M.S. run time 0:00:03.546955\n",
      "[14/04/2022 18:15:46] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 480us/step - loss: 1.5205 - accuracy: 0.5800\n",
      "10/10 [==============================] - 0s 460us/step - loss: 1.3765 - accuracy: 0.6267\n",
      "[14/04/2022 18:15:49] M.S. of ArticularyWordRecognition Multiple SR vsr  Units 100     IIRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:15:53] M.S. run time 0:00:03.543849\n",
      "[14/04/2022 18:15:53] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 643us/step - loss: 1.0521 - accuracy: 0.8267\n",
      "10/10 [==============================] - 0s 389us/step - loss: 0.9378 - accuracy: 0.8000\n",
      "[14/04/2022 18:15:56] M.S. of ArticularyWordRecognition Multiple SR Multiple IS  Units 100      IRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:16:00] M.S. run time 0:00:03.934451\n",
      "[14/04/2022 18:16:00] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 506us/step - loss: 1.6385 - accuracy: 0.6300\n",
      "10/10 [==============================] - 0s 554us/step - loss: 1.6374 - accuracy: 0.6033\n",
      "[14/04/2022 18:16:03] M.S. of ArticularyWordRecognition Multiple SR Multiple IS  Units 100     IIRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:16:06] M.S. run time 0:00:03.669156\n",
      "[14/04/2022 18:16:06] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 441us/step - loss: 1.2169 - accuracy: 0.6333\n",
      "10/10 [==============================] - 0s 426us/step - loss: 1.0960 - accuracy: 0.6833\n",
      "[14/04/2022 18:16:09] M.S. of ArticularyWordRecognition Multiple SR Multiple IS vsr  Units 100      IRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:16:13] M.S. run time 0:00:03.767955\n",
      "[14/04/2022 18:16:13] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 459us/step - loss: 1.3661 - accuracy: 0.6567\n",
      "10/10 [==============================] - 0s 430us/step - loss: 1.3937 - accuracy: 0.6167\n",
      "[14/04/2022 18:16:16] M.S. of ArticularyWordRecognition Multiple SR Multiple IS vsr  Units 100     IIRESN\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "[14/04/2022 18:16:21] M.S. run time 0:00:04.677411\n",
      "[14/04/2022 18:16:21] Running 2 benchmarks\n",
      "10/10 [==============================] - 0s 490us/step - loss: 2.8488 - accuracy: 0.2100\n",
      "10/10 [==============================] - 0s 473us/step - loss: 2.7937 - accuracy: 0.2867\n",
      "Requested time: 0:01:28.837128\n"
     ]
    }
   ],
   "source": [
    "from plotting import plot_model\n",
    "\n",
    "datasets = config.get('Datasets')\n",
    "classes = config.get('Classes')\n",
    "\n",
    "run_time = time()\n",
    "for dataset_name in datasets:\n",
    "    train_path = os.path.join(DATA_ROOT, dataset_name, dataset_name + '_TRAIN.ts')\n",
    "    test_path = os.path.join(DATA_ROOT, dataset_name, dataset_name + '_TEST.ts')\n",
    "\n",
    "    x_train_all, y_train_all = load_sktime_dataset(train_path)\n",
    "    x_test, y_test = load_sktime_dataset(test_path)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all,\n",
    "                                                      test_size=0.33, random_state=42, stratify=y_train_all)\n",
    "\n",
    "    train_set = (x_train.astype(np.float32), y_train)\n",
    "    val_set = (x_val.astype(np.float32), y_val)\n",
    "    test_set = (x_test.astype(np.float32), y_test)\n",
    "\n",
    "    features = x_train.shape[-1]\n",
    "    output_units = len(np.unique(y_test))  # Dataset must have one of each features\n",
    "\n",
    "    for class_name in classes:\n",
    "        for experiment_name, hps in config.get(class_name).get(\"Experiments\").items():\n",
    "            for model_fn in config.get(class_name).get(\"Models\"):\n",
    "                model_name = get_name(model_fn)\n",
    "                print(\"[{}] M.S. of {: >25} {: >15} {: >10} {: >10}\".\n",
    "                      format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"),\n",
    "                             dataset_name, class_name, experiment_name, model_name))\n",
    "                already_tested = benchmarks.is_benchmarked(dataset_name, class_name, experiment_name, model_name)\n",
    "                if already_tested and SKIP:\n",
    "                    print(\"[                   ] Skip Already tested!\")\n",
    "                    continue\n",
    "                start_model = time()\n",
    "                build_fn = partial(model_fn, output_units, features, hps)\n",
    "                names = (dataset_name, class_name, experiment_name, model_name)\n",
    "\n",
    "                tuner, score = model_selection(build_fn, names,\n",
    "                                               train_set, val_set,\n",
    "                                               tuner_path=TUNER_ROOT, verbose=MS_VERBOSE)\n",
    "\n",
    "                duration = time() - start_model\n",
    "                string_out = \"[\" + datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") + \"] M.S. run time \" + str(\n",
    "                    timedelta(seconds=duration))\n",
    "                print(string_out)\n",
    "\n",
    "                if BENCHMARKS_TRIALS > 0:\n",
    "                    model, stat = testing_model(names, tuner, train_set, val_set, test_set)\n",
    "                    stat.add_score(score)\n",
    "                    benchmarks.add(dataset_name, class_name, experiment_name, model_name, stat)\n",
    "                    plot_model(model, names, path=WEIGHTS_ROOT, show=False)\n",
    "\n",
    "                benchmarks.save()\n",
    "    benchmarks.save()\n",
    "duration = time() - run_time\n",
    "string_out = \"Requested time: \" + str(timedelta(seconds=duration))\n",
    "print(string_out)\n",
    "send_notification(\"All Done\", string_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%time\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}