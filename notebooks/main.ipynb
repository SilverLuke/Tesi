{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/statsmodels/tsa/base/tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/datatypes/_series/_check.py:42: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/datatypes/_panel/_check.py:45: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_INDEX_TYPES = (pd.Int64Index, pd.RangeIndex, pd.PeriodIndex, pd.DatetimeIndex)\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/datatypes/_panel/_check.py:46: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  VALID_MULTIINDEX_TYPES = (pd.Int64Index, pd.RangeIndex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Data dir: /dati/luca/Uni-Luca/Tesi/progetto/datasets\n",
      "      Tuner dir: /dati/luca/Uni-Luca/Tesi/progetto/models\n",
      "Tensorboard dir: /tmp/tensorboard\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from functools import *\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.getcwd() + os.sep + os.pardir)\n",
    "sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "from IRESNs_tensorflow.time_series_datasets import *\n",
    "from IRESNs_tensorflow.models import *\n",
    "from benchmarks import *\n",
    "import notify2  # TODO replace notify watch here : https://notify2.readthedocs.io/en/latest/\n",
    "from tensorflow import keras\n",
    "\n",
    "DATA_ROOT = os.path.join(PROJECT_ROOT, \"datasets\")\n",
    "TUNER_ROOT = os.path.join(PROJECT_ROOT, \"models\")\n",
    "BENCHMARKS_ROOT = os.path.join(PROJECT_ROOT, \"plots\", \"benchmarks\")\n",
    "WEIGHTS_ROOT = os.path.join(PROJECT_ROOT, \"plots\", \"weights\")\n",
    "TB_ROOT = os.path.join(os.path.abspath(os.sep), \"tmp\", \"tensorboard\")\n",
    "\n",
    "BENCHMARKS_DIR = \"benchmarks\"\n",
    "WEIGHTS_DIR = \"weights\"\n",
    "\n",
    "print(\"       Data dir:\", DATA_ROOT)\n",
    "print(\"      Tuner dir:\", TUNER_ROOT)\n",
    "print(\"Tensorboard dir:\", TB_ROOT)\n",
    "\n",
    "TUNER = \"RandomSearch\"  # \"Hyperband\" or \"BayesianOptimization\" or \"RandomSearch\"\n",
    "\n",
    "MAX_TRIALS = 500  # How many configuration the tuner will try\n",
    "MAX_EPOCHS = 500  # How many epochs the tuner train the model for each trials\n",
    "TRIALS = 1  # Positive Integer. How many iterations for one set of hyperparameters\n",
    "PATIENCE = 10 # EarlyStopping\n",
    "BENCHMARKS_TRIALS = 10  # How many times do the benchmark. 0 to skip BENCHMARKS\n",
    "\n",
    "MAX_UNITS = 250\n",
    "UNITS = sorted([50, 75, 100, 150, 250])\n",
    "MINVAL = 0.01\n",
    "MAXVAL = 1.5\n",
    "\n",
    "SKIP = True  # Skip if a model is already tested?\n",
    "OVERWRITE = False  # Redo the model selection for a model?\n",
    "\n",
    "READOUT_ACTIVATION_BINARY = keras.activations.sigmoid\n",
    "LOSS_FUNCTION_BINARY = keras.losses.BinaryCrossentropy()\n",
    "READOUT_ACTIVATION = keras.activations.softmax  # https://www.tensorflow.org/api_docs/python/tf/keras/activations\n",
    "LOSS_FUNCTION = keras.losses.SparseCategoricalCrossentropy()  # https://www.tensorflow.org/api_docs/python/tf/keras/losses\n",
    "\n",
    "if not os.path.exists(TUNER_ROOT):\n",
    "    os.makedirs(TUNER_ROOT)\n",
    "if not os.path.exists(TB_ROOT):\n",
    "    os.makedirs(TB_ROOT)\n",
    "\n",
    "TUNER_STRING = TUNER + \".\" + str(MAX_EPOCHS) + \"me\" + str(MAX_TRIALS) + \"mt.testFinale\"\n",
    "\n",
    "benchmarks = BenchmarksDB(load_path=os.path.join(BENCHMARKS_ROOT, \"benchmarks.\"+ TUNER_STRING +\".json\"), plot_path=WEIGHTS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|        | ArticularyWordRecognition | CharacterTrajectories | Epilepsy | JapaneseVowels  | Libras | SpokenArabicDigits |\n",
    "|--------|:-------------------------:|:---------------------:|:--------:|:---------------:|:------:|:------------------:|\n",
    "| Input  |             9             |           3           |    3     |       12        |   2    |         13         |\n",
    "| Output |            25             |          20           |    4     |        9        |   15   |         10         |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "from keras_tuner import Hyperband, BayesianOptimization, RandomSearch\n",
    "from keras import callbacks as kc\n",
    "\n",
    "class CurrentMSModel(kc.Callback):\n",
    "    def __init__(self, names):\n",
    "        super().__init__()\n",
    "        self.dataset_name, self.class_name, self.experiment_name, self.model_name = names\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        #ids.clear_output(wait=True)\n",
    "        print(\"MS of {} {} {} {}\".format(self.dataset_name, self.class_name, self.experiment_name, self.model_name))\n",
    "\n",
    "\n",
    "import hashlib\n",
    "def get_seed(exp_name, mod_name):\n",
    "    union = exp_name + mod_name\n",
    "    a = hashlib.md5(union.encode('UTF-8'))\n",
    "    return int(a.hexdigest(), 16)\n",
    "\n",
    "\n",
    "def model_selection(build_model_fn, names,\n",
    "                    train_set, val_set,\n",
    "                    tuner_path, verbose=1):\n",
    "    dataset_name, class_name, experiment_name, model_name = names\n",
    "    x_train, y_train = train_set\n",
    "    x_val, y_val = val_set\n",
    "    seed = get_seed(experiment_name, model_name)\n",
    "    if TUNER == \"Hyperband\":\n",
    "        working_dir = os.path.join(tuner_path, TUNER_STRING, dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = Hyperband(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            hyperband_iterations=1.,\n",
    "            seed=seed,\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    elif TUNER == \"BayesianOptimization\":\n",
    "        working_dir = os.path.join(tuner_path, TUNER_STRING, dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = BayesianOptimization(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=MAX_TRIALS,\n",
    "            seed=seed,\n",
    "\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    elif TUNER == \"RandomSearch\":\n",
    "        working_dir = os.path.join(tuner_path, TUNER_STRING, dataset_name, class_name)\n",
    "        if not os.path.exists(working_dir):\n",
    "            os.makedirs(working_dir)\n",
    "\n",
    "        tuner = RandomSearch(\n",
    "            build_model_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_trials=MAX_TRIALS,\n",
    "            seed=seed,\n",
    "\n",
    "            directory=working_dir,\n",
    "            project_name=experiment_name + ' ' + model_name,\n",
    "            overwrite=OVERWRITE,\n",
    "            executions_per_trial=TRIALS,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Unknown Tuner -> {}\".format(TUNER))\n",
    "\n",
    "    # now the tuner will search the best hyperparameters\n",
    "    tuner.search(x_train, y_train, epochs=MAX_EPOCHS, validation_data=(x_val, y_val),\n",
    "                 callbacks=[\n",
    "                     keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE),\n",
    "                     #CurrentMSModel(names)\n",
    "                 ], verbose=verbose)\n",
    "\n",
    "    return tuner, tuner.oracle.get_best_trials(1)[0].score\n",
    "\n",
    "def testing_model(names, tuner,\n",
    "                  train_set, val_set, test_set,\n",
    "                  tensorboard_path=None, benchmarks_verbose=0):\n",
    "    dataset_name, class_name, experiment_name, model_name = names\n",
    "    x_train, y_train = train_set\n",
    "    x_val, y_val = val_set\n",
    "    x_test, y_test = test_set\n",
    "\n",
    "    # keras.callbacks.CallbackList([])\n",
    "    callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)]\n",
    "    if tensorboard_path is not None:\n",
    "        tensorboard_dir = tensorboard_path + model_name\n",
    "        callbacks.append(keras.callbacks.TensorBoard(tensorboard_dir, profile_batch='500,500'))\n",
    "\n",
    "    print(\"[{}] Running {} benchmarks\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), BENCHMARKS_TRIALS))\n",
    "\n",
    "    best_model_hp = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "    test_model = None\n",
    "\n",
    "    required_time = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "    test_acc = []\n",
    "\n",
    "\n",
    "    tf.random.set_seed(int(hash((experiment_name, model_name))))\n",
    "\n",
    "    for i in range(BENCHMARKS_TRIALS):\n",
    "        initial_time = time()\n",
    "\n",
    "        test_model = tuner.hypermodel.build(best_model_hp)\n",
    "        history = test_model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=MAX_EPOCHS,\n",
    "                       callbacks=callbacks, verbose=benchmarks_verbose)\n",
    "        test_loss, accuracy = test_model.evaluate(x_test, y_test)\n",
    "\n",
    "        required_time.append(time() - initial_time)\n",
    "\n",
    "        train_acc.append(history.history['accuracy'][-1])\n",
    "        val_acc.append(history.history['val_accuracy'][-1])\n",
    "        test_acc.append(accuracy)\n",
    "\n",
    "    stat = Statistic(best_model_hp, train_acc, val_acc, test_acc, required_time)\n",
    "\n",
    "    return test_model, stat\n",
    "\n",
    "\n",
    "notify2_init = False\n",
    "\n",
    "def send_notification(title, message):\n",
    "    def notify2_init_fun():\n",
    "        global notify2_init\n",
    "        if not notify2_init:\n",
    "            notify2.init(\"Tesi\")\n",
    "\n",
    "    notify2_init_fun()\n",
    "\n",
    "    notice = notify2.Notification(title, message)\n",
    "    notice.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Local Hyperparameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This cell is a little nightmare\n",
    "\n",
    "class HP:\n",
    "    FREE = 0\n",
    "    FIXED = 1\n",
    "    RESTRICTED = 2\n",
    "\n",
    "    def __init__(self, hp_type, value=None):\n",
    "        self.value = value\n",
    "        self.type = hp_type\n",
    "\n",
    "    @classmethod\n",
    "    def free(cls):\n",
    "        return cls(HP.FREE)\n",
    "\n",
    "    @classmethod\n",
    "    def fixed(cls, value):\n",
    "        return cls(HP.FIXED, value)\n",
    "\n",
    "    @classmethod\n",
    "    def restricted(cls, value=None):\n",
    "        return cls(HP.RESTRICTED, value)\n",
    "\n",
    "\n",
    "def get_units(tuner, name, hp, min_value, max_value, sampling=None):\n",
    "    if hp.type == HP.FREE:\n",
    "        tmp = tuner.Int(name, min_value=min_value, max_value=max_value, sampling=sampling)\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        tmp = tuner.Choise(name, UNITS, ordered=True)\n",
    "    elif hp.type == HP.FIXED:\n",
    "        tmp = tuner.Fixed(name, hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_float(tuner, name, hp, min_value, max_value, sampling=None, pn=None, pv=None):\n",
    "    if hp.type == HP.FREE or (hp.type == HP.RESTRICTED and hp.value is None):\n",
    "        tmp = tuner.Float(name, min_value=min_value, max_value=max_value, sampling=sampling, parent_name=pn,\n",
    "                          parent_values=pv)\n",
    "    elif hp.type == HP.FIXED or (hp.type == HP.RESTRICTED and hp.value is not None):\n",
    "        tmp = tuner.Fixed(name, hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_bool(tuner, name, hp):\n",
    "    if hp.type == HP.FREE or hp.type == HP.RESTRICTED:\n",
    "        tmp = tuner.Boolean(name)\n",
    "    elif hp.type == HP.FIXED:\n",
    "        tmp = tuner.Fixed(name, hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_float_vec(tuner, name, hp, length, min_value, max_value, step=None, sampling=None, pn=None, pv=None):\n",
    "    if hp.type == HP.FREE:\n",
    "        tmp = [tuner.Float(name + ' ' + str(i), min_value=min_value, max_value=max_value, sampling=sampling,\n",
    "                           parent_name=pn, parent_values=pv, step=step)\n",
    "               for i in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        tmp = [tuner.Fixed(name + ' ' + str(i), hp.value[i]) for i in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        tmp2 = tuner.Float(name + ' 0', min_value=min_value, max_value=max_value, sampling=sampling, parent_name=pn,\n",
    "                               parent_values=pv, step=step)\n",
    "        tmp = [tmp2 for _ in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_connectivity_esn1(tuner, hp):\n",
    "    if hp.type == HP.FREE:\n",
    "        tmp = tuner.Float('connectivity 0', min_value=0.1, max_value=1., sampling=\"linear\")\n",
    "    elif hp.type == HP.FIXED:\n",
    "        raise ValueError(\"HP.FIXED not valid for connectivity\")\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        tmp = tuner.Fixed('connectivity 0', hp.value)\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_connectivity_esn2(tuner, hp, length):\n",
    "    if hp.type == HP.FREE:\n",
    "        tmp = [tuner.Float('connectivity ' + str(i), min_value=0., max_value=1., sampling=\"linear\") for i in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        raise ValueError(\"HP.FIXED not valid for connectivity\")\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        if hp.value is None:\n",
    "            tmp2 = tuner.Float('connectivity 0', min_value=0., max_value=1., sampling=\"linear\")\n",
    "        else:\n",
    "            tmp2 = tuner.Fixed('connectivity 0', hp.value)\n",
    "        tmp = [tmp2 for _ in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return tmp\n",
    "\n",
    "def get_connectivity(tuner, hp, length):  # It is a squared matrix\n",
    "    if hp.type == HP.FREE:\n",
    "        conn_matrix = [[tuner.Float('connectivity ' + str(i), min_value=0., max_value=1., sampling=\"linear\") if i == j else\n",
    "                        tuner.Float('connectivity ' + str(i) + '->' + str(j), min_value=0., max_value=1., sampling=\"linear\")\n",
    "                        for i in range(length)]\n",
    "                       for j in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        diagonal, off_diagonal = hp.value\n",
    "        off_diagonal = tuner.Fixed('connectivity X->Y', off_diagonal)\n",
    "        conn_matrix = [[tuner.Fixed('connectivity ' + str(i), diagonal) if i == j else\n",
    "                        off_diagonal\n",
    "                        for i in range(length)]\n",
    "                       for j in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        if hp.value is None:\n",
    "            connectivity = [tuner.Float('connectivity ' + str(i), min_value=0., max_value=1., sampling=\"linear\") for i in range(length)]\n",
    "        else:\n",
    "            tmp = tuner.Fixed('connectivity 0', hp.value)\n",
    "            connectivity = [ tmp for _ in range(length)]\n",
    "        intra_connectivity = tuner.Float('connectivity X->Y', min_value=0., max_value=1., sampling=\"linear\")\n",
    "        conn_matrix = [[connectivity[i] if i == j else intra_connectivity for i in range(length)] for j in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return conn_matrix\n",
    "\n",
    "def get_minmax(tuner, hp, length):  # Is a matrix for ESN3 and ESN4\n",
    "    if hp.type == HP.FREE:\n",
    "        minmax_vec = [[0. if i == j else tuner.Float('minmax ' + str(i) + '->' + str(j), min_value=MINVAL, max_value=MAXVAL, sampling=\"linear\")\n",
    "                   for i in range(length)]\n",
    "                  for j in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        minmax = tuner.Float('minmax', min_value=MINVAL, max_value=MAXVAL, sampling=\"linear\")\n",
    "        minmax_vec = [[0. if i == j else minmax\n",
    "                   for i in range(length)]\n",
    "                  for j in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        minmax = hp.value\n",
    "        minmax_vec = [[0. if i == j else minmax\n",
    "                   for i in range(length)]\n",
    "                  for j in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return minmax_vec\n",
    "\n",
    "def get_spectral_radius(tuner, hp, length):  # Is a vector\n",
    "    if hp.type == HP.FREE:\n",
    "        sr_vec = [tuner.Float('spectral radius ' + str(i), min_value=MINVAL, max_value=MAXVAL, sampling=\"linear\") for i in range(length)]\n",
    "    elif hp.type == HP.RESTRICTED:\n",
    "        spectral_radius = tuner.Float('spectral radius 0', min_value=MINVAL, max_value=MAXVAL, sampling=\"linear\")\n",
    "        sr_vec = [spectral_radius for _ in range(length)]\n",
    "    elif hp.type == HP.FIXED:\n",
    "        spectral_radius = hp.value\n",
    "        sr_vec = [spectral_radius for _ in range(length)]\n",
    "    else:\n",
    "        raise ValueError(\"HP type not found\")\n",
    "    return sr_vec\n",
    "\n",
    "def get_gsr(tuner, gsr):\n",
    "    global_sr = get_bool(tuner, 'use G.S.R.', gsr)\n",
    "    if global_sr:\n",
    "        global_sr = tuner.Float('G.S.R.', min_value=MINVAL, max_value=MAXVAL, sampling=\"linear\", parent_name='use G.S.R.', parent_values=True)\n",
    "    else:\n",
    "        global_sr = None\n",
    "\n",
    "    return global_sr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build model functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def build_ESN(output_classes, _reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, _gsr, _off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,  # Defined by experiment\n",
    "               tuner) -> ESN:\n",
    "    if output_classes == 2:\n",
    "        output_units = 1\n",
    "        readout_activation = READOUT_ACTIVATION_BINARY\n",
    "        loss = LOSS_FUNCTION_BINARY\n",
    "    else:\n",
    "        output_units = output_classes\n",
    "        readout_activation = READOUT_ACTIVATION\n",
    "        loss = LOSS_FUNCTION\n",
    "\n",
    "    tmp_model = ESN(units=get_units(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     connectivity=get_connectivity_esn1(tuner, connectivity),\n",
    "                     spectral_radius=get_float(tuner, 'spectral radius 0', spectral_radius, min_value=MINVAL,\n",
    "                                               max_value=MAXVAL, sampling=\"linear\"),\n",
    "                     output_units=output_units,\n",
    "                     readout_activation=readout_activation,\n",
    "                     input_scaling=get_float(tuner, 'input scaling 0', input_scaling, min_value=MINVAL,\n",
    "                                             max_value=MAXVAL, sampling=\"linear\"),\n",
    "                     bias_scaling=get_float(tuner, 'bias scaling 0', bias_scaling, min_value=MINVAL, max_value=MAXVAL,\n",
    "                                            sampling=\"linear\"),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., sampling=\"linear\"),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_IRESN(output_classes, reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, gsr, _off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,\n",
    "               # Defined by experiment\n",
    "               tuner) -> IRESN:\n",
    "    if output_classes == 2:\n",
    "        output_units = 1\n",
    "        readout_activation = READOUT_ACTIVATION_BINARY\n",
    "        loss = LOSS_FUNCTION_BINARY\n",
    "    else:\n",
    "        output_units = output_classes\n",
    "        readout_activation = READOUT_ACTIVATION\n",
    "        loss = LOSS_FUNCTION\n",
    "\n",
    "    tmp_model = IRESN(units=get_units(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     sub_reservoirs=reservoirs,\n",
    "                     connectivity=get_connectivity_esn2(tuner, connectivity, reservoirs),\n",
    "                     spectral_radius=get_spectral_radius(tuner, spectral_radius, reservoirs),\n",
    "                     gsr=get_gsr(tuner, gsr),\n",
    "                     output_units=output_units,\n",
    "                     readout_activation=readout_activation,\n",
    "                     input_scaling=get_float(tuner, 'input scaling 0', input_scaling, min_value=MINVAL,\n",
    "                                             max_value=MAXVAL, sampling=\"linear\"),\n",
    "                     bias_scaling=get_float(tuner, 'bias scaling 0', bias_scaling, min_value=MINVAL, max_value=MAXVAL,\n",
    "                                            sampling=\"linear\"),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., sampling=\"linear\"),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_IIRESN(output_classes, reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, gsr, off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,\n",
    "               # Defined by experiment\n",
    "               tuner) -> IIRESN:\n",
    "\n",
    "    if output_classes == 2:\n",
    "        output_units = 1\n",
    "        readout_activation = READOUT_ACTIVATION_BINARY\n",
    "        loss = LOSS_FUNCTION_BINARY\n",
    "    else:\n",
    "        output_units = output_classes\n",
    "        readout_activation = READOUT_ACTIVATION\n",
    "        loss = LOSS_FUNCTION\n",
    "\n",
    "    tmp_model = IIRESN(units=get_units(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     sub_reservoirs=reservoirs,\n",
    "                     connectivity=get_connectivity(tuner, connectivity, reservoirs),\n",
    "                     spectral_radius=get_spectral_radius(tuner, spectral_radius, reservoirs),\n",
    "                     gsr=get_gsr(tuner, gsr),\n",
    "                     off_diagonal_limits=get_minmax(tuner, off_diag, reservoirs),\n",
    "                     output_units=output_units,\n",
    "                     readout_activation=readout_activation,\n",
    "                     input_scaling=get_float(tuner, 'input scaling 0', input_scaling, min_value=MINVAL,\n",
    "                                             max_value=MAXVAL, sampling=\"linear\"),\n",
    "                     bias_scaling=get_float(tuner, 'bias scaling 0', bias_scaling, min_value=MINVAL, max_value=MAXVAL,\n",
    "                                            sampling=\"linear\"),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., sampling=\"linear\"),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy'],\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "\n",
    "def build_IIRESNvsr(output_classes, reservoirs,  # Defined by dataset\n",
    "               units, spectral_radius, gsr, off_diag, connectivity, input_scaling, bias_scaling, leaky, learning_rate,\n",
    "               # Defined by experiment\n",
    "               tuner) -> IIRESNvsr:\n",
    "\n",
    "    if output_classes == 2:\n",
    "        output_units = 1\n",
    "        readout_activation = READOUT_ACTIVATION_BINARY\n",
    "        loss = LOSS_FUNCTION_BINARY\n",
    "    else:\n",
    "        output_units = output_classes\n",
    "        readout_activation = READOUT_ACTIVATION\n",
    "        loss = LOSS_FUNCTION\n",
    "\n",
    "    partitions = [tuner.Float('partition ' + str(i), min_value=0.1, max_value=1.0, sampling=\"linear\") for i in range(reservoirs)]\n",
    "    total = sum(partitions)\n",
    "    # Normalize the partition vector now sum(partitions) == 1.\n",
    "    partitions = list(map(lambda _x: 0 if total == 0 else _x / total, partitions))\n",
    "\n",
    "    tmp_model = IIRESNvsr(units=get_units(tuner, 'units', units, 50, MAX_UNITS),\n",
    "                     sub_reservoirs=reservoirs,\n",
    "                     connectivity=get_connectivity(tuner, connectivity, reservoirs),\n",
    "                     partitions=partitions,\n",
    "                     spectral_radius=get_spectral_radius(tuner, spectral_radius, reservoirs),\n",
    "                     gsr=get_gsr(tuner, gsr),\n",
    "                     off_diagonal_limits=get_minmax(tuner, off_diag, reservoirs),\n",
    "                     output_units=output_units,\n",
    "                     readout_activation=readout_activation,\n",
    "                     input_scaling=get_float(tuner, 'input scaling 0', input_scaling, min_value=MINVAL,\n",
    "                                             max_value=MAXVAL, sampling=\"linear\"),\n",
    "                     bias_scaling=get_float(tuner, 'bias scaling 0', bias_scaling, min_value=MINVAL, max_value=MAXVAL,\n",
    "                                            sampling=\"linear\"),\n",
    "                     leaky=get_float(tuner, 'leaky', leaky, min_value=0.0, max_value=1., sampling=\"linear\"),\n",
    "                     )\n",
    "\n",
    "    alpha = get_float(tuner, 'learning rate', learning_rate, min_value=1e-5, max_value=1e-1, sampling='log')\n",
    "    tmp_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(alpha),  # keras.optimizers.RMSprop(alpha),\n",
    "        loss=loss,\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return tmp_model\n",
    "\n",
    "def get_name(fn):\n",
    "    return fn.__annotations__['return'].__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confiugrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'Datasets': [\n",
    "        #\"CharacterTrajectories\",\n",
    "        #\"Libras\",\n",
    "        \"SpokenArabicDigits\",\n",
    "        #\"ArticularyWordRecognition\",\n",
    "        #\"Epilepsy\",\n",
    "        #\"JapaneseVowels\"\n",
    "    ],\n",
    "    'Classes': [\n",
    "        #'Free Interconnectivity',\n",
    "        'Multiple S.R.',\n",
    "        'Single S.R.'\n",
    "    ],\n",
    "    'Models': [\n",
    "        build_ESN,\n",
    "        build_IRESN,\n",
    "        build_IIRESN,\n",
    "        build_IIRESNvsr\n",
    "    ],\n",
    "    'Skip': {\n",
    "        get_name(build_ESN)  : ['Multiple S.R.', 'Single S.R.'],  # Experiments name of those classes must be equals\n",
    "        #get_name(build_ESN1_2): ['Multiple S.R.', 'Single S.R.'],\n",
    "    },\n",
    "    #                Units         | Spectral radius | use G.S.R.    | off-diag       | Connectivity     | Input scaling  | Bias scaling   | leaky     | learning rate\n",
    "    #                Fre-Res       | Fre-Res         | Fre-fix-res   | Fre-Res        | Fre-Res          | Fre-Res        | Fre-Res        |Fre-fix-res|Fre-fix-res\n",
    "    'Free Interconnectivity': {\n",
    "        'All':      (HP.free(),     HP.free(),       HP.free(),       HP.free(),       HP.free(),         HP.free(),       HP.free(),       HP.free(), HP.free())\n",
    "    },\n",
    "    'Multiple S.R.': {\n",
    "        #    'Units': (HP.restricted(), HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        # 'Units 50': (HP.fixed(50),  HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        # 'Units 75': (HP.fixed(75),  HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 100': (HP.fixed(100), HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        #'Units 150': (HP.fixed(150), HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        #'Units 250': (HP.fixed(250), HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "    },\n",
    "    'Single S.R.': {\n",
    "        #    'Units': (HP.restricted(),  HP.free(),       HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        # 'Units 50': (HP.fixed(50),  HP.restricted(), HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        # 'Units 75': (HP.fixed(75),  HP.restricted(), HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        'Units 100': (HP.fixed(100), HP.restricted(), HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        #'Units 150': (HP.fixed(150), HP.restricted(), HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "        #'Units 250': (HP.fixed(250), HP.restricted(), HP.fixed(False), HP.restricted(), HP.restricted(1.), HP.restricted(), HP.restricted(), HP.free(), HP.free()),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/utils/data_io.py:63: FutureWarning: This function has moved to datasets/_data_io, this version will be removed in V0.10\n",
      "  warn(\n",
      "/dati/luca/Uni-Luca/Tesi/progetto/venv/lib/python3.9/site-packages/sktime/utils/data_io.py:63: FutureWarning: This function has moved to datasets/_data_io, this version will be removed in V0.10\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17/03/2022 11:48:40] M.S. of        SpokenArabicDigits   Multiple S.R.  Units 100        ESN\n",
      "[                   ] Skip Already tested!\n",
      "[17/03/2022 11:48:40] M.S. of        SpokenArabicDigits   Multiple S.R.  Units 100     IIRESN\n",
      "[                   ] Skip Already tested!\n",
      "Requested time: 0:00:09.049946\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "datasets = config.get('Datasets')\n",
    "classes = config.get('Classes')\n",
    "models_fn = config.get('Models')\n",
    "skip = config.get('Skip')\n",
    "\n",
    "run_time = time()\n",
    "for dataset_name in datasets:\n",
    "    train_path = os.path.join(DATA_ROOT, dataset_name, dataset_name + '_TRAIN.ts')\n",
    "    test_path = os.path.join(DATA_ROOT, dataset_name, dataset_name + '_TEST.ts')\n",
    "\n",
    "    x_train_all, y_train_all = load_sktime_dataset(train_path)\n",
    "    x_test, y_test = load_sktime_dataset(test_path)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all,\n",
    "                                                      test_size=0.33, random_state=42, stratify=y_train_all)\n",
    "\n",
    "    train_set = (x_train, y_train)\n",
    "    val_set = (x_val, y_val)\n",
    "    test_set = (x_test, y_test)\n",
    "\n",
    "    features = x_train.shape[-1]\n",
    "    output_units = len(np.unique(y_test))  # Dataset must have one of each features\n",
    "\n",
    "    for class_name in classes:\n",
    "        for experiment_name, params in config.get(class_name).items():\n",
    "            for model_fn in models_fn:\n",
    "                model_name = get_name(model_fn)\n",
    "                print(\"[{}] M.S. of {: >25} {: >15} {: >10} {: >10}\".format(datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\"), dataset_name, class_name, experiment_name,\n",
    "                                                              model_name))\n",
    "                already_tested = benchmarks.is_benchmarked(dataset_name, class_name, experiment_name, model_name)\n",
    "                if already_tested and SKIP:\n",
    "                    print(\"[                   ] Skip Already tested!\")\n",
    "                    continue\n",
    "                start_model = time()\n",
    "                build_fn = partial(model_fn, output_units, features, *params)\n",
    "                names = (dataset_name, class_name, experiment_name, model_name)\n",
    "\n",
    "                tuner, score = model_selection(build_fn, names,\n",
    "                                        train_set, val_set,\n",
    "                                        tuner_path=TUNER_ROOT, verbose=0)\n",
    "\n",
    "                duration = time() - start_model\n",
    "                string_out = \"[\"+datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\") +\"] M.S. run time \" + str(timedelta(seconds=duration))\n",
    "                print(string_out)\n",
    "\n",
    "                if BENCHMARKS_TRIALS > 0:\n",
    "                    model, stat = testing_model(names, tuner, train_set, val_set, test_set)\n",
    "                    stat.add_score(score)\n",
    "                    benchmarks.add(dataset_name, class_name, experiment_name, model_name, stat)\n",
    "                    plot_model(model, names, path=WEIGHTS_ROOT, show=False)\n",
    "\n",
    "                if model_name in skip.keys():  # Il modello è nei duplicati\n",
    "                    equals_classes = skip[model_name].copy()\n",
    "                    if class_name in equals_classes:  # Anche la classe è nei duplicati\n",
    "                        equals_classes.remove(class_name)\n",
    "                        stat = benchmarks.get(dataset_name, class_name, experiment_name, model_name)\n",
    "                        if stat is not None:\n",
    "                            benchmarks.copy_stats(dataset_name, equals_classes, experiment_name, model_name, stat)\n",
    "\n",
    "                        source_dir = os.path.join(TUNER_ROOT, TUNER_STRING, dataset_name, class_name, experiment_name + ' ' + model_name)\n",
    "                        for to_copy_classes in equals_classes:\n",
    "                            dest_dir = os.path.join(TUNER_ROOT, TUNER_STRING, dataset_name, to_copy_classes)\n",
    "                            if not os.path.exists(dest_dir):\n",
    "                                os.makedirs(dest_dir)\n",
    "                            dest_dir = os.path.join(dest_dir, experiment_name + ' ' + model_name)\n",
    "                            if not os.path.exists(dest_dir):\n",
    "                                os.symlink(source_dir, dest_dir)\n",
    "                benchmarks.save()\n",
    "\n",
    "benchmarks.save()\n",
    "\n",
    "duration = time() - run_time\n",
    "string_out = \"Requested time: \" + str(timedelta(seconds=duration))\n",
    "print(string_out)\n",
    "send_notification(\"All Done\", string_out)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%time\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}